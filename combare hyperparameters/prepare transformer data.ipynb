{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\r\n",
    "This Jupyter Notebook is an integral component of a bachelor thesis that aims to explore the capabilities of specialized chatbots, particularly those built using BERT. The notebook is designed to preprocess and analyze a set of CSV files. These files contain performance metrics from various machine learning models, including BERT-based chatbots, to provide a comprehensive view of their effectiveness and efficiency."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import required libraries\r\n",
    "In this section, we import all the necessary libraries that will be used throughout the notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "# !pip install pandas\r\n",
    "import pandas as pd\r\n",
    "import glob\r\n",
    "import ast"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading and Preprocessing\r\n",
    "\r\n",
    "Here, we define the path to the folder containing the CSV files and read them into Pandas DataFrames. We also preprocess the data by adding new columns for learning rate and batch size, extracted from the filenames."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the path to the folder containing the CSV files\r\n",
    "path = 'files_transformer'  # Update this path as needed\r\n",
    "\r\n",
    "# Read all CSV files in the folder\r\n",
    "all_files = glob.glob(f\"{path}/*.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Loop through each file to read and preprocess the data\r\n",
    "for filename in all_files:\r\n",
    "    # Read the CSV file into a DataFrame\r\n",
    "    temp_df = pd.read_csv(filename)\r\n",
    "    \r\n",
    "    # Extract learning rate and batch size from the filename\r\n",
    "    learning_rate = filename.split('\\\\')[-1].split('LR ')[1].split(' ')[0]\r\n",
    "    batch_size = ['16', '32', '64']\r\n",
    "    \r\n",
    "    # Add new columns for learning rate and batch size\r\n",
    "    temp_df['learning_rate'] = learning_rate\r\n",
    "    temp_df['batch_size'] = batch_size\r\n",
    "    \r\n",
    "    # Append the DataFrame to the list\r\n",
    "    data_frames.append(temp_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Concatenate all DataFrames into a single DataFrame\r\n",
    "final_df = pd.concat(data_frames, ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Filter and explode the DataFrame\r\n",
    "final_df = final_df.explode(['Epochs', 'Training Loss', 'Validation Loss', 'Training Accuracy', 'Validation Accuracy', 'Test Loss (every 10 epochs)', 'Test Accuracy (every 10 epochs)'])\r\n",
    "final_df = final_df[(final_df['Epochs'] % 5 == 0) & (final_df['Epochs'] < 30)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Column Renaming and Saving\r\n",
    "Finally, we rename the columns to be more descriptive and save the transformed data into a new CSV file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Rename columns\r\n",
    "column_rename_dict = {\r\n",
    "    'Epochs': 'epoch',\r\n",
    "    'Training Accuracy': 'train_accuracy',\r\n",
    "    'Training Loss': 'train_loss',\r\n",
    "    'Validation Accuracy': 'val_accuracy',\r\n",
    "    'Validation Loss': 'val_loss',\r\n",
    "    'Test Accuracy (every 10 epochs)': 'test_accuracy'\r\n",
    "}\r\n",
    "final_df.rename(columns=column_rename_dict, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Reorder columns\r\n",
    "final_df = final_df[['batch_size', 'epoch', 'learning_rate', 'train_accuracy', 'train_loss', 'val_accuracy', 'val_loss', 'test_accuracy']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "df.to_csv('transformer_data.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}