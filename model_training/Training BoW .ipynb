{"cells":[{"cell_type":"markdown","metadata":{"id":"uWuh0ETJWHk7"},"source":["# Training BoW\n","This Jupyter Notebook is part of a bachelor thesis that aims to investigate the capabilities of specialized chatbots. Specifically, we will train a Bag-of-Words (BoW) model and evaluate its performance."]},{"cell_type":"markdown","metadata":{},"source":["# Import Libraries\n","We import all the necessary libraries for data manipulation, machine learning, and visualization.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import Libraries\n","import nltk\n","import numpy as np\n","import tensorflow as tf\n","import tflearn\n","import random\n","import os\n","import json\n","import pickle\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import GermanStemmer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","from sklearn.model_selection import train_test_split\n","from tflearn.data_utils import to_categorical, pad_sequences\n","import datetime\n"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preparation\n","We load the data from Google Drive and prepare it for training."]},{"cell_type":"markdown","metadata":{},"source":["# Mount Google Drive (Only for Google Colab, comment this out if running locally)\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","# Change this path if you are running the notebook locally\n","path = '/content/drive/My Drive/Colab Notebooks/Chatbot'\n","\n","# Load the training data\n","# Change this path if you are running the notebook locally\n","df = pd.read_csv('{path}/train.csv')\n","\n","# Load the test data\n","df_test = pd.read_csv('{path}/test.csv')\n"]},{"cell_type":"markdown","metadata":{},"source":["# Text Preprocessing\n","We tokenize the utterances and remove stopwords."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize the German stemmer and stopwords\n","stemmer = GermanStemmer()\n","stop_words = stopwords.words('german')\n","ignore_words = ['?', '.', ',', '='] + stop_words\n","\n","# Tokenize the utterances and remove stopwords\n","df['tokenized_utterances'] = df['utterances'].apply(lambda x: nltk.word_tokenize(x, language='german'))\n","df['filtered_utterances'] = df['tokenized_utterances'].apply(lambda x: [word for word in x if word.lower() not in ignore_words])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to flatten a nested list\n","def flatten_list(nested_list):\n","    result = []\n","    for element in nested_list:\n","        if isinstance(element, list):\n","            result.extend(flatten_list(element))\n","        else:\n","            result.append(element)\n","    return result\n","\n","# Create a Bag-of-Words (BoW) representation\n","all_words = flatten_list(df['filtered_utterances'])\n","all_words = [stemmer.stem(w.lower()) for w in all_words]\n","unique_words = sorted(list(set(all_words)))\n","classes = sorted(list(set(df['intent'].values)))"]},{"cell_type":"markdown","metadata":{},"source":["We also create a Bag-of-Words representation for each utterance."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a numerical representation for each filtered utterance\n","def bow_representation(words_list):\n","    return [1 if word in words_list else 0 for word in unique_words]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['BoW'] = df['filtered_utterances'].apply(bow_representation)\n","df_test['BoW'] = df_test['words'].apply(lambda row: compute_bow(row, words))\n","# Show the DataFrame with the new columns\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize the LabelEncoder\n","le = LabelEncoder()\n","# Fit the LabelEncoder and transform the 'intent' column\n","df['intent_numerical'] = le.fit_transform(df['intent'])"]},{"cell_type":"markdown","metadata":{"id":"8CTQU5DTWfnU"},"source":["## Testing and Prediction Functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define synonyms for domain-specific terms\n","synonyms = {\n","    'ordner': 'akte',\n","    'dateien': 'dokument',\n","    'vorgang': 'register'\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to preprocess a question\n","def preprocess_question(question):\n","    tokenized_words = nltk.word_tokenize(question, language='german')\n","    ignore_words = set([stemmer.stem(word.lower()) for word in ignore_words])\n","    processed_words = [stemmer.stem(synonyms.get(word.lower(), word).lower()) for word in tokenized_words]\n","    return [word for word in processed_words if word not in ignore_words]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to generate Bag-of-Words representation\n","def generate_bow(question, words):\n","    processed_words = preprocess_question(question)\n","    return np.array(bow_representation(processed_words, words))"]},{"cell_type":"code","execution_count":101,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1691563644212,"user":{"displayName":"Luca Sailer","userId":"00809447999470188670"},"user_tz":-120},"id":"xxFZMOP2NHJm"},"outputs":[],"source":["ERROR_THRESHOLD = 0.30"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to classify a question\n","def classify_question(question, model):\n","    bow_array = generate_bow(question, unique_words)\n","    results = model.predict(np.array([bow_array]))[0]\n","    filtered_results = [(i, r) for i, r in enumerate(results) if r > ERROR_THRESHOLD]\n","    filtered_results.sort(key=lambda x: x[1], reverse=True)\n","    return [(classes[i], r) for i, r in filtered_results]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to predict the class of a question\n","def predict_class(question, model):\n","    predicted_class = classify_question(question, model)\n","    return predicted_class[0][0] if predicted_class else \"\""]},{"cell_type":"markdown","metadata":{},"source":["# Model Training\n","We define the architecture of the neural network and train it using different hyperparameters."]},{"cell_type":"code","execution_count":75,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1691563641515,"user":{"displayName":"Luca Sailer","userId":"00809447999470188670"},"user_tz":-120},"id":"rPMT1FndhhcW"},"outputs":[],"source":["# Get today's date as YYYYMMDD\n","today = datetime.date.today().strftime(\"%Y%m%d\")\n","# Identifier for PyTorch\n","identifier = \"tflearn\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize hyperparameters\n","learning_rates = [0.1, 0.01, 0.001]\n","batch_sizes = [16, 32, 64]\n","epochs = [i for i in range(100, 350, 50)]"]},{"cell_type":"code","execution_count":93,"metadata":{"executionInfo":{"elapsed":34,"status":"ok","timestamp":1691563644208,"user":{"displayName":"Luca Sailer","userId":"00809447999470188670"},"user_tz":-120},"id":"ZYnNZY2D1Ut8"},"outputs":[],"source":["# Shuffle the DataFrame\n","df = df.sample(frac=1, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert labels to binary vectors\n","y_categorical = to_categorical(df['intent_numerical'], nb_classes=len(set(df['intent_numerical'])))\n","\n","# Split data into training and val sets\n","X_train, X_val, y_train, y_val = train_test_split(df['BoW'].tolist(), y_categorical, test_size=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_path = f\"{path}/{today}_{identifier}_train_logs\""]},{"cell_type":"code","execution_count":108,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1691563644215,"user":{"displayName":"Luca Sailer","userId":"00809447999470188670"},"user_tz":-120},"id":"WkIUxtjF4xQt"},"outputs":[],"source":["# Custom callback to log metrics\n","class MetricLogger(tflearn.callbacks.Callback):\n","    def __init__(self):\n","        self.epoch_data = []\n","\n","    def on_epoch_end(self, training_state):\n","        data = {}\n","        # Getting metrics for training\n","        data['train_acc'] = training_state.acc_value\n","        data['train_loss'] = training_state.global_loss\n","\n","        # Getting metrics for validation\n","        if training_state.val_acc is not None:\n","            data['val_acc'] = training_state.val_acc\n","        if training_state.val_loss is not None:\n","            data['val_loss'] = training_state.val_loss\n","\n","        self.epoch_data.append(data)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to train and evaluate the model\n","def train_eval_model(learning_rate, epoch, batch_size):\n","    # Reset the default graph (important for retraining)\n","    tf.compat.v1.reset_default_graph()\n","    # Initialize the metric logger\n","    logger = MetricLogger()\n","    # Define the neural network architecture\n","    net = tflearn.input_data(shape=[None, len(X_train[0])])\n","    net = tflearn.fully_connected(net, 64)\n","    net = tflearn.fully_connected(net, 64)\n","    net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n","    net = tflearn.regression(net, learning_rate=learning_rate)\n","    \n","    # Initialize and train the model\n","    model = tflearn.DNN(net, tensorboard_dir=model_path)\n","    model.fit(X_train, y_train,validation_set=(X_val, y_val), n_epoch=epoch, batch_size=batch_size, show_metric=True,  callbacks=logger)\n","    # Convert epoch data to DataFrame\n","    df_metric = pd.DataFrame(logger.epoch_data)\n","    \n","    # Save the model\n","    model_path = f\"{path}/LR {learning_rate} BatchSize {batch_size} Epoch {epoch} model.tflearn\"\n","    model.save(model_path)\n","\n","    # If you only want the last epoch\n","    metrics = df_metric.iloc[-1]\n","    # Calculate the accuracy of the model\n","    df_test['actual'] = df_test['utterances'].apply(lambda row: predicted_class(row, model))\n","    metrics[\"test_acc\"] = accuracy_score(df_test['intent'], df_test['actual'])\n","    metrics[\"learning_rate\"]= learning_rate\n","    metrics[\"epoche\"]= epoch\n","    metrics[\"batch_size\"]= batch_size\n","    print(metrics)\n","    return metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_eval_results = []\n","for lr in learning_rates:\n","    for epoch in epochs:\n","        for batch in batch_sizes:\n","            result = train_eval_model(lr, epoch, batch)\n","            train_eval_results.append(result)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df=pd.DataFrame(train_eval_results)\n","df.to_csv( f\"/{path}/BoW_Hyperparameter_Results.csv\", index= False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save all of our data structures\n","pickle.dump({'words': words, 'classes': classes, 'train_x': X_train, 'train_y': y_train}, open(f\"{path}/{today}_{identifier}_train_data\", \"wb\"))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOSvzEu1jKfE+Oac+GKb6np","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.4"}},"nbformat":4,"nbformat_minor":2}
