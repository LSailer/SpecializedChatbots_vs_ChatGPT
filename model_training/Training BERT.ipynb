{"cells":[{"cell_type":"markdown","metadata":{"id":"yksKhXMOhN0U"},"source":["# Train with Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34760,"status":"ok","timestamp":1692293563010,"user":{"displayName":"Luca Sailer","userId":"00809447999470188670"},"user_tz":-120},"id":"kA1eIjnZgrcp"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}],"source":["# !pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8WpzV2e9hQ3G"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from transformers import BertTokenizer, TFBertForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preparation\n","We load the data from Google Drive and prepare it for training.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Note: If running locally, comment out the next two lines\n","from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Code Cell: Data Preparation\n","path = /content/drive/My Drive/Colab Notebooks/Chatbot\n","data_path = f'{path}/train-without-Prep.csv'\n","test_data = pd.read_csv(f'{path}/test-without-Prep.csv')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pd.read_csv(data_path).sample(frac=1, random_state=42)\n","train_texts, test_texts, train_labels, test_labels = train_test_split(data['utterances'], data['intent'], test_size=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ullqWmwqnmQ"},"outputs":[],"source":["test_texts_callback = test_data['utterances']\n","test_labels_callback = pd.Categorical(test_data['intent']).codes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Konvertiere Kategorien in numerische Werte\n","train_labels = pd.Categorical(train_labels).codes\n","test_labels = pd.Categorical(test_labels).codes"]},{"cell_type":"markdown","metadata":{},"source":["# Tokenization and Model Initialization\n","We initialize the BERT tokenizer and model, and tokenize the training and test data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Code Cell: Tokenization and Model Initialization\n","tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n","train_labels = pd.Categorical(train_labels).codes\n","test_labels = pd.Categorical(test_labels).codes\n","\n","train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, return_tensors=\"tf\")\n","test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, return_tensors=\"tf\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Custom Callback and Training Function\n","We define a custom callback to log test metrics and a function to train the model with different hyperparameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ChOo4c0q3nU"},"outputs":[],"source":["class TestCallback(tf.keras.callbacks.Callback):\n","    def __init__(self, test_data):\n","        self.test_data = test_data\n","        self.test_history = []\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        if (epoch + 1) % 5 == 0:\n","            x, y = self.test_data\n","            loss, accuracy = self.model.evaluate(x, y, verbose=0)\n","            self.test_history.append({\"epoch\": epoch + 1, \"test_loss\": loss, \"test_accuracy\": accuracy})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AocgtlAtqxlO"},"outputs":[],"source":["# Function to Train and Evaluate Model\n","def train_eval_model(learning_rate, epochs, batch_size):\n","    # Initialize the model and optimizer\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    model = TFBertForSequenceClassification.from_pretrained('bert-base-german-cased', num_labels=len(data['intent'].unique()))\n","    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n","    \n","    # Reset the default graph (useful for multiple training runs)\n","    tf.compat.v1.reset_default_graph()\n","    \n","    # Train the model\n","    history = model.fit(\n","        [train_encodings.input_ids, train_encodings.attention_mask], np.array(train_labels),\n","        validation_data=([test_encodings.input_ids, test_encodings.attention_mask], np.array(test_labels)),\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        callbacks=[test_callback]\n","    )\n","    \n","    # Collect and return training and validation metrics\n","    results = {\n","        \"Epochs\": list(range(1, epochs + 1)),\n","        \"Training Loss\": history.history[\"loss\"],\n","        \"Validation Loss\": history.history[\"val_loss\"],\n","        \"Training Accuracy\": history.history[\"accuracy\"],\n","        \"Validation Accuracy\": history.history[\"val_accuracy\"],\n","        \"Test Loss (every 5 epochs)\": [None] * epochs,\n","        \"Test Accuracy (every 5 epochs)\": [None] * epochs\n","    }\n","    \n","    # Add test metrics from the custom callback\n","    for test_result in test_callback.test_history:\n","        epoch = test_result[\"epoch\"]\n","        results[\"Test Loss (every 5 epochs)\"][epoch - 1] = test_result[\"test_loss\"]\n","        results[\"Test Accuracy (every 5 epochs)\"][epoch - 1] = test_result[\"test_accuracy\"]\n","    \n","    return results"]},{"cell_type":"markdown","metadata":{},"source":["# Hyperparameter Tuning\n","In this section, we perform hyperparameter tuning by training the model with different combinations of learning rates, batch sizes, and epochs. The performance metrics for each combination are saved to a CSV file."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define hyperparameters to tune\n","learning_rates = [0.1, 0.01, 0.001, 0.0001]\n","batch_sizes = [16, 32, 64]\n","epoche = 25"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize the custom callback with the separate test dataset\n","test_callback = TestCallback(([test_encodings_callback.input_ids, test_encodings_callback.attention_mask], np.array(test_labels_callback)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Initialize list to collect training evaluations\n","trainings_eval = []\n","\n","# Loop through all combinations of hyperparameters\n","i = 0\n","for learning_rate in learning_rates:\n","    for batch_size in batch_sizes:\n","        i += 1\n","        print(f'Run {i}: Learning Rate = {learning_rate}, Batch Size = {batch_size}')\n","        \n","        # Train and evaluate the model\n","        training_result = train_eval_model(learning_rate, epoche, batch_size)\n","        \n","        # Append the result to the list\n","        trainings_eval.append(training_result)\n","        \n","        # Save intermediate results to CSV\n","        df = pd.DataFrame(trainings_eval)\n","        df.to_csv(f\"{path}/BERT_Hyperparameter_Results_LR_{learning_rate}_BS_{batch_size}.csv\", index=False)\n","\n","# Save final results to CSV\n","df = pd.DataFrame(trainings_eval)\n","df.to_csv(\"{path}/BERT_Hyperparameter_Results.csv\", index=False)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMTB8p5uWT2mCy37yXOP/Up","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
